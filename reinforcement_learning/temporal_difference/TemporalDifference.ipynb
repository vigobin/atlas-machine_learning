{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HyBI9WBrZ1wZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"Monte Carlo function\"\"\"\n",
        "\n",
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100,\n",
        "                alpha=0.1, gamma=0.99):\n",
        "    \"\"\"Performs the Monte Carlo algorithm:\n",
        "        env is the openAI environment instance.\n",
        "        V is a numpy.ndarray of shape (s,) containing the value estimate.\n",
        "        policy is a function that takes in a state and returns the\n",
        "            next action to take.\n",
        "        episodes is the total number of episodes to train over.\n",
        "        max_steps is the maximum number of steps per episode.\n",
        "        alpha is the learning rate.\n",
        "        gamma is the discount rate.\n",
        "        Returns: V, the updated value estimate.\"\"\"\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        states = []\n",
        "        while not done and steps < max_steps:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        G = 0\n",
        "        for t in range(steps-1, -1, -1):\n",
        "            G = gamma * G + rewards[t]\n",
        "            V[states[t]] = V[states[t]] + alpha * (G - V[states[t]])\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "# monte_carlo = __import__('0-monte_carlo').monte_carlo\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=2)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmXQveArakRT",
        "outputId": "cf23f8ae-a965-49e3-f7c9-ce23f4d93be2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8.78e-02  6.08e-02  3.38e-02  4.68e-02  8.12e-02  3.29e-05  6.10e-02\n",
            "   1.43e-02]\n",
            " [ 4.69e-02  3.84e-02  4.29e-13  5.64e-03  9.84e-02  3.46e-02  6.94e-02\n",
            "   4.35e-02]\n",
            " [ 4.38e-44  3.62e-44  6.37e-49 -1.00e+00  1.23e-01  1.30e-01  2.42e-01\n",
            "   1.39e-01]\n",
            " [ 2.14e-26  4.49e-35  6.77e-27  4.16e-12  2.98e-07 -1.00e+00  2.35e-01\n",
            "   9.48e-02]\n",
            " [ 3.07e-20  1.75e-94  3.42e-71 -1.00e+00  7.46e-04  4.11e-04  1.61e-01\n",
            "   1.72e-01]\n",
            " [ 5.20e-23 -1.00e+00 -1.00e+00  9.00e-01  2.49e-02  3.94e-02 -1.00e+00\n",
            "   2.58e-01]\n",
            " [ 4.16e-17 -1.00e+00  1.67e-01  6.56e-01 -1.00e+00  3.26e-01 -1.00e+00\n",
            "   2.83e-01]\n",
            " [ 1.88e-07  4.05e-05  7.07e-03 -1.00e+00  1.00e+00  7.45e-01  8.29e-01\n",
            "   1.00e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"TD(位) function\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def td_lambtha(env, V, policy, lambtha, episodes=5000,\n",
        "               max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"Performs the TD(位) algorithm:\n",
        "        env is the openAI environment instance.\n",
        "        V is a numpy.ndarray of shape (s,) containing the value estimate.\n",
        "        policy is a function that takes in a state and returns the\n",
        "            next action to take.\n",
        "        lambtha is the eligibility trace factor.\n",
        "        episodes is the total number of episodes to train over.\n",
        "        max_steps is the maximum number of steps per episode.\n",
        "        alpha is the learning rate.\n",
        "        gamma is the discount rate.\n",
        "        Returns: V, the updated value estimate.\"\"\"\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        eligibility_trace = np.zeros_like(V)\n",
        "        while not done and steps < max_steps:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            delta = reward + gamma * V[next_state] * (not done) - V[state]\n",
        "            eligibility_trace[state] += 1.0\n",
        "            V += alpha * delta * eligibility_trace\n",
        "            eligibility_trace *= gamma * lambtha\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "zaCrE_QOTWkT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "# td_lambtha = __import__('1-td_lambtha').td_lambtha\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ-1DyTCTZk7",
        "outputId": "2fb0ed4c-c814-437c-fe79-5f60f54e1f25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7.7129e-03  1.3165e-02  2.0520e-02  3.4019e-02  3.8642e-02  6.9335e-02\n",
            "   6.3121e-02  7.1144e-02]\n",
            " [ 3.5873e-03  6.1755e-03  7.4042e-03  2.8975e-02  3.9403e-02  8.4272e-02\n",
            "   7.3803e-02  8.4687e-02]\n",
            " [ 1.7886e-03  2.1251e-03  1.8724e-03 -1.0000e+00  2.8866e-02  7.7807e-02\n",
            "   6.0959e-02  9.9125e-02]\n",
            " [ 7.5919e-04  6.5023e-04  1.3745e-03  3.0894e-03  6.5835e-03 -1.0000e+00\n",
            "   3.6495e-02  5.7337e-02]\n",
            " [ 1.1376e-03  2.3054e-04  2.5522e-04 -1.0000e+00  1.6921e-02  6.5772e-02\n",
            "   5.9346e-02  1.0212e-01]\n",
            " [ 8.7576e-04 -1.0000e+00 -1.0000e+00  6.1363e-01  6.4190e-02  1.0289e-01\n",
            "  -1.0000e+00  2.8864e-01]\n",
            " [ 1.4933e-03 -1.0000e+00  9.8777e-02  3.4497e-01 -1.0000e+00  3.5278e-01\n",
            "  -1.0000e+00  3.1137e-01]\n",
            " [ 2.6897e-03  8.1614e-03  2.4684e-02 -1.0000e+00  1.0000e+00  5.9940e-01\n",
            "   9.2261e-01  1.0000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SARSA(位) Function\"\"\"\n",
        "\n",
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
        "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    \"\"\"Performs SARSA(位):\n",
        "        env is the openAI environment instance.\n",
        "        Q is a numpy.ndarray of shape (s,a) containing the Q table.\n",
        "        lambtha is the eligibility trace factor.\n",
        "        episodes is the total number of episodes to train over.\n",
        "        max_steps is the maximum number of steps per episode.\n",
        "        alpha is the learning rate.\n",
        "        gamma is the discount rate.\n",
        "        epsilon is the initial threshold for epsilon greedy.\n",
        "        min_epsilon is the minimum value that epsilon should decay to.\n",
        "        epsilon_decay is the decay rate for updating epsilon between episodes.\n",
        "        Returns: Q, the updated Q table.\"\"\"\n",
        "    n_actions = env.action_space.n\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        action = epsilon_greedy(Q, state, n_actions, epsilon)\n",
        "        eligibility_trace = np.zeros_like(Q)\n",
        "        for _ in range(max_steps):\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            new_action = epsilon_greedy(Q, new_state, n_actions, epsilon)\n",
        "            delta = reward + gamma * Q[new_state, new_action] * (not done) - Q[state, action]\n",
        "            eligibility_trace[state, action] += 1.0\n",
        "            Q += alpha * delta * eligibility_trace\n",
        "            eligibility_trace *= gamma * lambtha\n",
        "            if done:\n",
        "                break\n",
        "            state, action = new_state, new_action\n",
        "        epsilon = max(epsilon - epsilon_decay, min_epsilon)\n",
        "    return Q\n",
        "\n",
        "def epsilon_greedy(Q, state, n_actions, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = np.random.randint(n_actions)\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    return action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yosEtEgygwnm",
        "outputId": "9d2bce20-772e-4bd6-c954-1623d0308b1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "# sarsa_lambtha = __import__('2-sarsa_lambtha').sarsa_lambtha\n",
        "\n",
        "np.random.seed(0)\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "Q = np.random.uniform(size=(64, 4))\n",
        "np.set_printoptions(precision=4)\n",
        "print(sarsa_lambtha(env, Q, 0.9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El_e8imSg0U6",
        "outputId": "7046a104-fac1-405d-d7ae-6ebd337b39d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.008  0.0087 0.0084 0.0074]\n",
            " [0.0093 0.0086 0.0087 0.0093]\n",
            " [0.0099 0.0117 0.0075 0.01  ]\n",
            " [0.0101 0.0119 0.0135 0.0109]\n",
            " [0.0154 0.0154 0.0146 0.0153]\n",
            " [0.0195 0.0193 0.0152 0.0194]\n",
            " [0.0206 0.0216 0.0237 0.0233]\n",
            " [0.0208 0.0243 0.0206 0.0222]\n",
            " [0.0081 0.0085 0.0078 0.0086]\n",
            " [0.0081 0.0087 0.0081 0.0085]\n",
            " [0.0094 0.0091 0.0099 0.0097]\n",
            " [0.0086 0.0097 0.0103 0.0118]\n",
            " [0.0147 0.0138 0.0142 0.0148]\n",
            " [0.018  0.018  0.0163 0.0184]\n",
            " [0.0215 0.0223 0.0226 0.0191]\n",
            " [0.024  0.0238 0.022  0.0216]\n",
            " [0.0072 0.0069 0.0071 0.0071]\n",
            " [0.0066 0.007  0.0069 0.0068]\n",
            " [0.0066 0.0067 0.0064 0.0072]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.0109 0.0136 0.0155 0.0135]\n",
            " [0.0182 0.0152 0.0137 0.0179]\n",
            " [0.0237 0.0249 0.026  0.0222]\n",
            " [0.0262 0.0235 0.0245 0.0238]\n",
            " [0.0068 0.0064 0.0069 0.0063]\n",
            " [0.0034 0.0057 0.0061 0.0062]\n",
            " [0.0056 0.0058 0.0061 0.006 ]\n",
            " [0.0021 0.0079 0.0026 0.0039]\n",
            " [0.0139 0.0123 0.0131 0.0061]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.0195 0.0289 0.0314 0.0247]\n",
            " [0.0331 0.03   0.0318 0.0194]\n",
            " [0.0046 0.0056 0.0063 0.0058]\n",
            " [0.0047 0.0043 0.0036 0.0059]\n",
            " [0.004  0.0017 0.0019 0.0026]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.0105 0.0108 0.0165 0.0122]\n",
            " [0.0235 0.0249 0.0216 0.0247]\n",
            " [0.0256 0.022  0.0241 0.0319]\n",
            " [0.0545 0.0508 0.0554 0.0527]\n",
            " [0.0061 0.0039 0.0066 0.0056]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.0472 0.0167 0.0502 0.0469]\n",
            " [0.019  0.0166 0.0183 0.0203]\n",
            " [0.032  0.032  0.0281 0.0374]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [0.2037 0.0475 0.1449 0.1399]\n",
            " [0.0083 0.0078 0.0085 0.0082]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.1449 0.1383 0.143  0.1328]\n",
            " [0.0247 0.0672 0.1795 0.1691]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.0754 0.0747 0.0207 0.0779]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.2509 0.2354 0.4783 0.1726]\n",
            " [0.0135 0.0125 0.0125 0.0095]\n",
            " [0.0183 0.014  0.0159 0.0189]\n",
            " [0.0405 0.0214 0.0374 0.0363]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.5073 0.1175 0.4656 0.1189]\n",
            " [0.3128 0.2914 0.2955 0.2455]\n",
            " [0.1334 0.4881 0.3556 0.7799]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    }
  ]
}
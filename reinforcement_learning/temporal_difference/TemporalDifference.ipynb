{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HyBI9WBrZ1wZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"Monte Carlo function\"\"\"\n",
        "\n",
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100,\n",
        "                alpha=0.1, gamma=0.99):\n",
        "    \"\"\"Performs the Monte Carlo algorithm:\n",
        "        env is the openAI environment instance.\n",
        "        V is a numpy.ndarray of shape (s,) containing the value estimate.\n",
        "        policy is a function that takes in a state and returns the\n",
        "            next action to take.\n",
        "        episodes is the total number of episodes to train over.\n",
        "        max_steps is the maximum number of steps per episode.\n",
        "        alpha is the learning rate.\n",
        "        gamma is the discount rate.\n",
        "        Returns: V, the updated value estimate.\"\"\"\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        rewards = []\n",
        "        states = []\n",
        "        while not done and steps < max_steps:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        G = 0\n",
        "        for t in range(steps-1, -1, -1):\n",
        "            G = gamma * G + rewards[t]\n",
        "            V[states[t]] = V[states[t]] + alpha * (G - V[states[t]])\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "# monte_carlo = __import__('0-monte_carlo').monte_carlo\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=2)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmXQveArakRT",
        "outputId": "cf23f8ae-a965-49e3-f7c9-ce23f4d93be2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8.78e-02  6.08e-02  3.38e-02  4.68e-02  8.12e-02  3.29e-05  6.10e-02\n",
            "   1.43e-02]\n",
            " [ 4.69e-02  3.84e-02  4.29e-13  5.64e-03  9.84e-02  3.46e-02  6.94e-02\n",
            "   4.35e-02]\n",
            " [ 4.38e-44  3.62e-44  6.37e-49 -1.00e+00  1.23e-01  1.30e-01  2.42e-01\n",
            "   1.39e-01]\n",
            " [ 2.14e-26  4.49e-35  6.77e-27  4.16e-12  2.98e-07 -1.00e+00  2.35e-01\n",
            "   9.48e-02]\n",
            " [ 3.07e-20  1.75e-94  3.42e-71 -1.00e+00  7.46e-04  4.11e-04  1.61e-01\n",
            "   1.72e-01]\n",
            " [ 5.20e-23 -1.00e+00 -1.00e+00  9.00e-01  2.49e-02  3.94e-02 -1.00e+00\n",
            "   2.58e-01]\n",
            " [ 4.16e-17 -1.00e+00  1.67e-01  6.56e-01 -1.00e+00  3.26e-01 -1.00e+00\n",
            "   2.83e-01]\n",
            " [ 1.88e-07  4.05e-05  7.07e-03 -1.00e+00  1.00e+00  7.45e-01  8.29e-01\n",
            "   1.00e+00]]\n"
          ]
        }
      ]
    }
  ]
}
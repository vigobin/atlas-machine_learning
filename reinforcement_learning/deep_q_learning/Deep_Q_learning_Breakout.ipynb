{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "if tf.__version__!='2.11.0':\n",
        "  !pip install tensorflow==2.11.0 --quiet\n",
        "  os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3VYtDbyTGQHa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2 --quiet\n",
        "!pip install gym[atari] --quiet\n",
        "!pip install atari-py --quiet"
      ],
      "metadata": {
        "id": "421IRnW4E1ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b89d248-ef51-41e6-d8f2-2f66ce41247c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m672.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.6/540.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTpYIqfiT46y",
        "outputId": "54ef9af6-38f3-48b5-be3d-655ec1e9b77a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m atari_py.import_roms /content/drive/MyDrive/dqn/roms/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iClXzJ-CGThZ",
        "outputId": "4325cf62-9bbc-4f25-8b2d-05812df22112"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying breakout.bin from /content/drive/MyDrive/dqn/roms/Breakout - Breakaway IV.bin to /usr/local/lib/python3.10/dist-packages/atari_py/atari_roms/breakout.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs.registration import register"
      ],
      "metadata": {
        "id": "s-yks8zfQ4gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082362f7-af67-4207-a33e-68bf1a42b5d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
            "  ROMS = resolve_roms()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "register(\n",
        "    id='Breakout-v4',\n",
        "    entry_point='gym.envs.atari:AtariEnv',\n",
        "    kwargs={'game': 'breakout', 'obs_type': 'image', 'frameskip': 1},\n",
        "    max_episode_steps=10000,\n",
        "    nondeterministic=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvL-_VeMQ5SP",
        "outputId": "a444b884-a284-4886-ac41-d82fb0de3558"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Breakout-v4\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "metadata": {
        "id": "oSyhN6TjJ-Hm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = 'Breakout-v4'\n",
        "\n",
        "# Get Environment.\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Testing to simplify the model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "policy = EpsGreedyQPolicy()\n",
        "memory = SequentialMemory(limit=1000000, window_length=1)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10000,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "# Define the callback\n",
        "class CustomCallback(Callback):\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        if step % 100 == 0:\n",
        "            print('Average reward at step {}: {}'.format(step, logs.get('episode_reward')))\n",
        "\n",
        "# Training and saving\n",
        "weights_filename = f'/content/drive/MyDrive/dqn/weights.h5'\n",
        "\n",
        "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2, callbacks=[CustomCallback()])\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT-TpoxOAj4c",
        "outputId": "bdebaafd-5221-429a-8e50-22906fa4b25c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/atari/environment.py:68: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "Average reward at step 0: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "  1056/100000: episode: 1, duration: 9.127s, episode steps: 1056, steps per second: 116, episode reward:  2.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.951 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "Average reward at step 1100: None\n",
            "  2216/100000: episode: 2, duration: 7.232s, episode steps: 1160, steps per second: 160, episode reward:  2.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.946 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "Average reward at step 1100: None\n",
            "Average reward at step 1200: None\n",
            "  3445/100000: episode: 3, duration: 10.270s, episode steps: 1229, steps per second: 120, episode reward:  3.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.964 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "  4480/100000: episode: 4, duration: 7.501s, episode steps: 1035, steps per second: 138, episode reward:  2.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.957 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "Average reward at step 1100: None\n",
            "Average reward at step 1200: None\n",
            "Average reward at step 1300: None\n",
            "  5786/100000: episode: 5, duration: 9.829s, episode steps: 1306, steps per second: 133, episode reward:  3.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.948 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "Average reward at step 1100: None\n",
            "  6922/100000: episode: 6, duration: 10.546s, episode steps: 1136, steps per second: 108, episode reward:  2.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.947 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "  8019/100000: episode: 7, duration: 7.492s, episode steps: 1097, steps per second: 146, episode reward:  3.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.942 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            "Average reward at step 900: None\n",
            "Average reward at step 1000: None\n",
            "Average reward at step 1100: None\n",
            "Average reward at step 1200: None\n",
            "  9248/100000: episode: 8, duration: 10.258s, episode steps: 1229, steps per second: 120, episode reward:  3.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.941 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n",
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "  9842/100000: episode: 9, duration: 3.824s, episode steps: 594, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.970 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
            "Average reward at step 0: None\n",
            "Average reward at step 100: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward at step 200: None\n",
            "Average reward at step 300: None\n",
            "Average reward at step 400: None\n",
            "Average reward at step 500: None\n",
            "Average reward at step 600: None\n",
            "Average reward at step 700: None\n",
            "Average reward at step 800: None\n",
            " 10694/100000: episode: 10, duration: 276.110s, episode steps: 852, steps per second:   3, episode reward:  2.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.231 [0.000, 3.000],  loss: 2879.371607, mae: 3.594421, mean_q: 4.867226\n",
            "Average reward at step 0: None\n",
            "done, took 365.483 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxAAP8x3Eefw",
        "outputId": "18f9d05a-0954-445e-be3d-74e99daea7a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 487\n",
            "Episode 2: reward: 0.000, steps: 486\n",
            "Episode 3: reward: 0.000, steps: 486\n",
            "Episode 4: reward: 0.000, steps: 486\n",
            "Episode 5: reward: 0.000, steps: 486\n",
            "Episode 6: reward: 0.000, steps: 486\n",
            "Episode 7: reward: 0.000, steps: 489\n",
            "Episode 8: reward: 0.000, steps: 486\n",
            "Episode 9: reward: 0.000, steps: 487\n",
            "Episode 10: reward: 0.000, steps: 486\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c85b05c66b0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_filename = f'/content/drive/MyDrive/dqn/weights.h5'\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF-PcJRp7rmC",
        "outputId": "9971e27f-5adb-4cfb-94b4-55af77d48283"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 10000\n",
            "Episode 2: reward: 0.000, steps: 10000\n",
            "Episode 3: reward: 0.000, steps: 10000\n",
            "Episode 4: reward: 0.000, steps: 10000\n",
            "Episode 5: reward: 0.000, steps: 10000\n",
            "Episode 6: reward: 0.000, steps: 10000\n",
            "Episode 7: reward: 0.000, steps: 10000\n",
            "Episode 8: reward: 0.000, steps: 10000\n",
            "Episode 9: reward: 0.000, steps: 10000\n",
            "Episode 10: reward: 0.000, steps: 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c85b0526ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}
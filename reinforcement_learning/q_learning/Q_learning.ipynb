{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import gym\n",
        "import gym.envs.toy_text.frozen_lake as fl\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "h758XQlfZYae"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X3qSyfQUYHdH"
      },
      "outputs": [],
      "source": [
        "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
        "    if desc is None and map_name is None:\n",
        "        desc = fl.generate_random_map(size=8)\n",
        "    env = fl.FrozenLakeEnv(desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load_frozen_lake = __import__('0-load_env').load_frozen_lake\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "env = load_frozen_lake()\n",
        "print(env.desc)\n",
        "print(env.P[0][0])\n",
        "env = load_frozen_lake(is_slippery=True)\n",
        "print(env.desc)\n",
        "print(env.P[0][0])\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "print(env.desc)\n",
        "env = load_frozen_lake(map_name='4x4')\n",
        "print(env.desc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAtiMbETZjwh",
        "outputId": "88851b32-64e0-4c0b-d11a-20c75cbac218"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
            " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
            "[(1.0, 0, 0.0, False)]\n",
            "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
            " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
            " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
            " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
            " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
            " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
            "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
            "[[b'S' b'F' b'F']\n",
            " [b'F' b'H' b'H']\n",
            " [b'F' b'F' b'G']]\n",
            "[[b'S' b'F' b'F' b'F']\n",
            " [b'F' b'H' b'F' b'H']\n",
            " [b'F' b'F' b'F' b'H']\n",
            " [b'H' b'F' b'F' b'G']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_init(env):\n",
        "    # The Q-table has a row for each state (env.observation_space.n)\n",
        "    # and a column for each action (env.action_space.n)\n",
        "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "gAvqFrNdcpnf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_frozen_lake = __import__('0-load_env').load_frozen_lake\n",
        "# q_init = __import__('1-q_init').q_init\n",
        "\n",
        "env = load_frozen_lake()\n",
        "Q = q_init(env)\n",
        "print(Q.shape)\n",
        "env = load_frozen_lake(is_slippery=True)\n",
        "Q = q_init(env)\n",
        "print(Q.shape)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "print(Q.shape)\n",
        "env = load_frozen_lake(map_name='4x4')\n",
        "Q = q_init(env)\n",
        "print(Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft_KGd8Zc3y8",
        "outputId": "8d05a62a-9560-46df-fb4e-8611c918651c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 4)\n",
            "(64, 4)\n",
            "(9, 4)\n",
            "(16, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Epsilon Greedy\"\"\"\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    \"\"\"Uses epsilon-greedy to determine the next action:\n",
        "    Q is a numpy.ndarray containing the q-table\n",
        "    state is the current state\n",
        "    epsilon is the epsilon to use for the calculation\n",
        "    Sample p with numpy.random.uniformn to determine if\n",
        "        algorithm should explore or exploit.\n",
        "    If exploring, pick the next action with numpy.random.randint\n",
        "        from all possible actions.\n",
        "    Returns: the next action index.\"\"\"\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action_index = np.random.randint(Q.shape[1])\n",
        "    else:\n",
        "        action_index = np.argmax(Q[state, :])\n",
        "\n",
        "    return action_index"
      ],
      "metadata": {
        "id": "eQcXk0ojfteD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_frozen_lake = __import__('0-load_env').load_frozen_lake\n",
        "# q_init = __import__('1-q_init').q_init\n",
        "# epsilon_greedy = __import__('2-epsilon_greedy').epsilon_greedy\n",
        "import numpy as np\n",
        "\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
        "np.random.seed(0)\n",
        "print(epsilon_greedy(Q, 7, 0.5))\n",
        "np.random.seed(1)\n",
        "print(epsilon_greedy(Q, 7, 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQjmu1UPfzAa",
        "outputId": "ebdbb869-0487-4368-c8da-510b52f90fff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, Q, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    total_rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state, :])\n",
        "\n",
        "            result = env.step(action)\n",
        "            next_state, reward, done, _ = result[:4]\n",
        "\n",
        "            Q[state, action] = Q[\n",
        "                state, action] + alpha * (\n",
        "                    reward + gamma * np.max(\n",
        "                        Q[next_state, :]) - Q[\n",
        "                            state, action])\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
        "\n",
        "    return Q, total_rewards"
      ],
      "metadata": {
        "id": "VlaVz73Jih1I"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_frozen_lake = __import__('0-load_env').load_frozen_lake\n",
        "# q_init = __import__('1-q_init').q_init\n",
        "# train = __import__('3-q_learning').train\n",
        "\n",
        "np.random.seed(0)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "\n",
        "Q, total_rewards  = train(env, Q)\n",
        "print(Q)\n",
        "split_rewards = np.split(np.array(total_rewards), 10)\n",
        "for i, rewards in enumerate(split_rewards):\n",
        "    print((i+1) * 500, ':', np.mean(rewards))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSiJOZAsimNE",
        "outputId": "c07d0421-9dc1-4a32-df99-ec6e5927a134"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.96059554 0.970299   0.95085698 0.96059579]\n",
            " [0.96058252 0.         0.03326856 0.25881872]\n",
            " [0.27480916 0.         0.         0.        ]\n",
            " [0.97029891 0.9801     0.         0.96059496]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.98009965 0.98009984 0.99       0.97029876]\n",
            " [0.98009741 0.98999927 1.         0.        ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "500 : 0.856\n",
            "1000 : 0.934\n",
            "1500 : 0.938\n",
            "2000 : 0.932\n",
            "2500 : 0.928\n",
            "3000 : 0.948\n",
            "3500 : 0.924\n",
            "4000 : 0.92\n",
            "4500 : 0.946\n",
            "5000 : 0.948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play(env, Q, max_steps=100):\n",
        "    state = env.reset()\n",
        "    total_rewards = 0\n",
        "    for step in range(max_steps):\n",
        "        action = np.argmax(Q[state, :])\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_rewards += reward\n",
        "        env.render()\n",
        "        if done:\n",
        "            break\n",
        "    return total_rewards"
      ],
      "metadata": {
        "id": "WFmkjW25dO0a"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_frozen_lake = __import__('0-load_env').load_frozen_lake\n",
        "# q_init = __import__('1-q_init').q_init\n",
        "# train = __import__('3-q_learning').train\n",
        "# play = __import__('4-play').play\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
        "env = load_frozen_lake(desc=desc)\n",
        "Q = q_init(env)\n",
        "\n",
        "Q, total_rewards  = train(env, Q)\n",
        "print(play(env, Q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "2JuV2r6GdWkj",
        "outputId": "e6b90fdb-7bd5-472b-f2af-e862fc481254"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5b713fe584d1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_rewards\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-6c6e5f13aad5>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, Q, max_steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    }
  ]
}